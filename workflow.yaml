# yaml-language-server: $schema=https://activate.parallel.works/workflow.schema.json
---
# Ray Cluster - Session workflow for SLURM/PBS/Controller execution
# Modify service scripts in scripts/ directory

permissions:
  - "*"
sessions:
  session:
    useTLS: false
    redirect: false

jobs:
  # =============================================================================
  # PREPROCESSING
  # =============================================================================
  preprocessing:
    steps:
      - name: Checkout service scripts
        uses: parallelworks/checkout
        ssh:
          remoteHost: ${{ inputs.resource.ip }}
        with:
          repo: https://github.com/parallelworks/activate-ray.git
          branch: main
          sparse_checkout:
            - scripts

      - name: Run controller setup
        ssh:
          remoteHost: ${{ inputs.resource.ip }}
        run: |
          set -e
          cd scripts
          export RAY_VERSION="${{ inputs.ray_version }}"
          export DASHBOARD_PORT="${{ inputs.dashboard_port }}"
          export RAY_PORT="${{ inputs.ray_port }}"
          export NUM_CPUS="${{ inputs.num_cpus }}"
          export NUM_GPUS="${{ inputs.num_gpus }}"
          export PW_SESSION_NAME="${{ sessions.session }}"
          bash setup.sh

  # =============================================================================
  # SESSION RUNNER - runs start.sh on compute node
  # =============================================================================
  session_runner:
    needs: [preprocessing]
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Submit session script
        early-cancel: any-job-failed
        uses: marketplace/job_runner/v4.0
        with:
          resource: ${{ inputs.resource }}
          rundir: "${PW_PARENT_JOB_DIR}"
          scheduler: ${{ inputs.submit_to_scheduler }}
          inject_markers: true
          slurm:
            is_disabled: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}
            account: ${{ inputs.slurm.account }}
            partition: ${{ inputs.slurm.partition }}
            qos: ${{ inputs.slurm.qos }}
            time: ${{ inputs.slurm.time }}
            nodes: ${{ inputs.slurm.nodes }}
            cpus_per_task: ${{ inputs.slurm.cpus_per_task }}
            gres: ${{ inputs.slurm.gres }}
            scheduler_directives: |
              ${{ inputs.slurm.scheduler_directives }}
          pbs:
            is_disabled: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'pbs' }}
            account: ${{ inputs.pbs.account }}
            queue: ${{ inputs.pbs.queue }}
            walltime: ${{ inputs.pbs.walltime }}
            select: ${{ inputs.pbs.select }}
            scheduler_directives: |
              ${{ inputs.pbs.scheduler_directives }}
          use_existing_script: true
          script_path: "${PW_PARENT_JOB_DIR}/scripts/start.sh"

      - name: Notify job ended
        early-cancel: any-job-failed
        run: touch job.ended

  # ==============================================================================
  # WAIT FOR SERVICE - wait for job to start and get connection info
  # ==============================================================================
  wait_for_service:
    needs: [preprocessing]
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Wait for service ready
        early-cancel: any-job-failed
        run: |
          set -e

          JOB_DIR="${PW_PARENT_JOB_DIR%/}"
          TIMEOUT=5
          INTERVAL=3
          MAX_ATTEMPTS=100

          echo "Waiting for Ray dashboard..."

          # Wait for job.started
          attempt=1
          while [ ! -f "${JOB_DIR}/job.started" ]; do
            [ -f "${JOB_DIR}/job.ended" ] && { echo "ERROR: Job ended before starting"; exit 1; }
            echo "[${attempt}/${MAX_ATTEMPTS}] Waiting for job.started..."
            sleep ${INTERVAL}
            ((attempt++))
            [ ${attempt} -gt ${MAX_ATTEMPTS} ] && { echo "ERROR: Timeout waiting for job"; exit 1; }
          done

          # Wait for HOSTNAME and SESSION_PORT files
          sleep 2
          for file in HOSTNAME SESSION_PORT; do
            attempt=1
            while [ ! -f "${JOB_DIR}/${file}" ]; do
              echo "Waiting for ${file}..."
              sleep 2
              ((attempt++))
              [ ${attempt} -gt 10 ] && { echo "ERROR: ${file} not found"; exit 1; }
            done
          done

          HOSTNAME=$(cat "${JOB_DIR}/HOSTNAME")
          SESSION_PORT=$(cat "${JOB_DIR}/SESSION_PORT")

          # Wait for dashboard to respond
          echo "Waiting for http://${HOSTNAME}:${SESSION_PORT}..."
          attempt=1
          while [ ${attempt} -le ${MAX_ATTEMPTS} ]; do
            if curl -s --connect-timeout ${TIMEOUT} "http://${HOSTNAME}:${SESSION_PORT}" -o /dev/null 2>&1; then
              echo "Ray Dashboard is ready!"
              echo "HOSTNAME=${HOSTNAME}" >> "${OUTPUTS}"
              echo "SESSION_PORT=${SESSION_PORT}" >> "${OUTPUTS}"
              exit 0
            fi
            [ -f "${JOB_DIR}/job.ended" ] && { echo "ERROR: Job ended"; exit 1; }
            echo "[${attempt}/${MAX_ATTEMPTS}] Dashboard not ready..."
            sleep ${INTERVAL}
            ((attempt++))
          done

          echo "ERROR: Dashboard timeout"
          exit 1

  # =============================================================================
  # UPDATE SESSION - configure session proxy
  # =============================================================================
  update_session:
    needs: [wait_for_service]
    ssh:
      remoteHost: ${{ inputs.resource.ip }}
    steps:
      - name: Get local port
        run: |
          local_port=$(pw agent open-port)
          echo "local_port=${local_port}" | tee -a $OUTPUTS

      - name: Update session
        uses: parallelworks/update-session
        with:
          target: ${{ inputs.resource.id }}
          name: ${{ sessions.session }}
          remoteHost: ${{ needs.wait_for_service.outputs.HOSTNAME }}
          remotePort: ${{ needs.wait_for_service.outputs.SESSION_PORT }}
          local_port: ${{ needs.update_session.outputs.local_port }}

  # =============================================================================
  # COMPLETE - print connection info
  # =============================================================================
  complete:
    needs: [update_session]
    steps:
      - name: Session ready
        run: |
          local_port="${{ needs.update_session.outputs.local_port }}"
          basepath="/me/session/${PW_USER}/${{ sessions.session }}"
          echo "=========================================="
          echo "Ray Dashboard Session is ready!"
          echo "=========================================="
          echo "  Proxy URL: https://${PW_PLATFORM_HOST}${basepath}/"
          echo "  Local tunnel: ssh -L ${local_port}:localhost:${local_port} -o ProxyCommand=\"pw ssh --proxy-command %h\" ${PW_USER}@workspace"
          echo "  Then access: http://localhost:${local_port}/"
          echo "=========================================="

"on":
  execute:
    inputs:
      resource:
        type: compute-clusters
        label: Service host
        include-workspace: false
        autoselect: true

      submit_to_scheduler:
        type: boolean
        label: Submit to Job Scheduler
        tooltip: Enable to submit job via SLURM or PBS scheduler (detected automatically from resource). Disable for direct SSH execution.
        default: false
        hidden: ${{ inputs.resource.schedulerType != 'slurm' && inputs.resource.schedulerType != 'pbs' }}

      ray_version:
        label: Ray Version
        type: string
        default: "2.40.0"
        tooltip: Ray version to install (e.g., 2.40.0, 2.10.0)

      ray_port:
        label: Ray Port
        type: number
        default: 6379
        hidden: true
        tooltip: Port for Ray head node GCS

      dashboard_port:
        label: Dashboard Port
        type: number
        default: 8265
        tooltip: Port for Ray dashboard web interface

      num_cpus:
        label: CPUs per Node
        type: number
        default: 0
        min: 0
        hidden: true
        tooltip: Number of CPUs to allocate per Ray node (0 = auto-detect)

      num_gpus:
        label: GPUs per Node
        type: number
        default: 0
        min: 0
        hidden: true
        tooltip: Number of GPUs to allocate per Ray node (0 = auto-detect)

      slurm:
        type: group
        label: SLURM Configuration
        hidden: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}
        collapsed: true
        items:
          is_disabled:
            type: boolean
            default: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}
            hidden: true

          account:
            label: Account
            type: slurm-accounts
            resource: ${{ inputs.resource }}
            tooltip: SLURM account for job submission (--account)
            optional: true
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}

          partition:
            type: slurm-partitions
            label: Partition
            optional: true
            resource: ${{ inputs.resource }}
            tooltip: Partition for job submission (--partition)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}

          qos:
            label: Quality of Service (QoS)
            type: slurm-qos
            resource: ${{ inputs.resource }}
            tooltip: SLURM QoS setting (--qos)
            optional: true
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}

          nodes:
            label: Number of Nodes
            type: number
            default: 2
            min: 1
            tooltip: Number of nodes for the Ray cluster (--nodes)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}

          cpus_per_task:
            type: number
            label: CPUs per Task
            min: 1
            max: 256
            default: 4
            tooltip: Number of CPUs for the job (--cpus-per-task)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}

          time:
            label: Walltime
            type: string
            default: 04:00:00
            tooltip: Maximum job duration, e.g., 04:00:00 (--time)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}

          gres:
            label: GPUs (gres)
            type: dropdown
            default: ""
            optional: true
            tooltip: GPU resource specification (--gres). Leave empty for CPU-only jobs.
            options:
              - value: ""
                label: "No GPU (CPU only)"
              - value: "gpu:1"
                label: "1 GPU (gpu:1)"
              - value: "gpu:2"
                label: "2 GPUs (gpu:2)"
              - value: "gpu:4"
                label: "4 GPUs (gpu:4)"
              - value: "gpu:8"
                label: "8 GPUs (gpu:8)"
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}

          scheduler_directives:
            type: editor
            label: Additional Directives
            optional: true
            tooltip: Additional SLURM directives (include #SBATCH prefix)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'slurm' }}

      pbs:
        type: group
        label: PBS Configuration
        hidden: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'pbs' }}
        collapsed: true
        items:
          is_disabled:
            type: boolean
            default: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'pbs' }}
            hidden: true

          account:
            label: Account
            type: string
            optional: true
            tooltip: PBS account for job submission (-A)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'pbs' }}

          queue:
            label: Queue
            type: string
            optional: true
            tooltip: PBS queue name (-q)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'pbs' }}

          walltime:
            label: Walltime
            type: string
            default: 04:00:00
            tooltip: Maximum job duration, e.g., 04:00:00 (-l walltime=)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'pbs' }}

          select:
            label: Resource Selection
            type: string
            default: "2:ncpus=4"
            placeholder: "2:ncpus=4:ngpus=1"
            tooltip: PBS resource selection string, e.g., 2:ncpus=4:ngpus=1 (-l select=)
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'pbs' }}

          scheduler_directives:
            label: Additional Directives
            type: editor
            tooltip: Additional PBS directives (include #PBS prefix)
            optional: true
            ignore: ${{ inputs.submit_to_scheduler != true || inputs.resource.schedulerType != 'pbs' }}
